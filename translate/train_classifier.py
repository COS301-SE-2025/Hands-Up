import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split # Still used for reference, though WLASL has its own split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.regularizers import l2 # Import for L2 regularization
from tqdm import tqdm # For progress bar

# --- Configuration (match with previous steps and new model params) ---
# IMPORTANT: This now points to the CSV generated by the data_preprocessor.py after augmentation
PROCESSED_DATA_CSV = 'wlasl_nslt_100_final_processed_data_augmented.csv'
PROCESSED_SEQUENCES_DIR = 'processed_sequences' # Directory where processed .npy files are saved
MODEL_SAVE_DIR = 'saved_models' # Directory to save trained models

# These should match the values used in data_preprocessor.py
SEQUENCE_LENGTH = 60
EXPECTED_COORDS_PER_FRAME = 1662 

# --- Model Hyperparameters ---
# Adjust these for performance vs. training time
LSTM_UNITS = 128 # Number of units in the LSTM layer. Keep it relatively small.
DENSE_UNITS = 128 # Number of units in the dense layer after LSTM.
DROPOUT_RATE = 0.5 # Increased from 0.3 to combat overfitting. You can try 0.5 too.
LEARNING_RATE = 0.0005
BATCH_SIZE = 32 # Adjust based on your RAM. Smaller batch = slower but less RAM.
EPOCHS = 100 # Max epochs. Early stopping will likely stop it sooner.

def load_data(df):
    """Loads processed landmark sequences and their corresponding labels."""
    X = [] # Features (landmark sequences)
    y = [] # Labels (gloss IDs)

    # Use tqdm to show progress as data is loaded
    for index, row in tqdm(df.iterrows(), total=len(df), desc="Loading processed data"):
        seq_path = row['processed_path']
        gloss_id = row['gloss_id']
        
        try:
            sequence = np.load(seq_path)
            # Ensure sequence has the correct shape before appending
            if sequence.shape == (SEQUENCE_LENGTH, EXPECTED_COORDS_PER_FRAME):
                X.append(sequence)
                y.append(gloss_id)
            else:
                print(f"Warning: Skipping {seq_path} due to incorrect shape: {sequence.shape}. Expected ({SEQUENCE_LENGTH}, {EXPECTED_COORDS_PER_FRAME})")
        except Exception as e:
            print(f"Error loading {seq_path}: {e}. Skipping.")
    
    return np.array(X), np.array(y)

def build_lstm_model(input_shape, num_classes, lstm_units, dense_units, dropout_rate, learning_rate):
    model = Sequential([
        LSTM(lstm_units, return_sequences=True, input_shape=input_shape, activation='tanh'),
        BatchNormalization(), # Add Batch Normalization
        Dropout(dropout_rate),

        LSTM(lstm_units, return_sequences=False, activation='tanh'),
        BatchNormalization(), # Add Batch Normalization
        Dropout(dropout_rate),

        Dense(dense_units, activation='relu', kernel_regularizer=l2(0.001)),
        BatchNormalization(), # Add Batch Normalization
        Dropout(dropout_rate),

        Dense(num_classes, activation='softmax', kernel_regularizer=l2(0.001))
    ])

    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
    return model
if __name__ == "__main__":
    os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

    if not os.path.exists(PROCESSED_DATA_CSV):
        print(f"Error: Processed data metadata CSV not found at {PROCESSED_DATA_CSV}.")
        print("Please ensure data_preprocessor.py has been run to create this file, including augmentation.")
        exit()
    
    df_final = pd.read_csv(PROCESSED_DATA_CSV)
    print(f"Loaded {len(df_final)} entries from {PROCESSED_DATA_CSV}")

    # Separate data by split
    train_df = df_final[df_final['split'] == 'train']
    val_df = df_final[df_final['split'] == 'val']
    test_df = df_final[df_final['split'] == 'test']

    print(f"Train samples (including augmented): {len(train_df)}")
    print(f"Validation samples: {len(val_df)}")
    print(f"Test samples: {len(test_df)}") # Changed to test_df as it's already filtered

    # Load data for each split
    X_train, y_train_ids = load_data(train_df)
    X_val, y_val_ids = load_data(val_df)
    X_test, y_test_ids = load_data(test_df)
    
    print(f"\nLoaded data shapes:")
    print(f"X_train: {X_train.shape}, y_train: {y_train_ids.shape}")
    print(f"X_val: {X_val.shape}, y_val: {y_val_ids.shape}")
    print(f"X_test: {X_test.shape}, y_test: {y_test_ids.shape}")

    # Determine number of classes from your processed data (should be 100 for NSLT-100)
    num_classes = df_final['gloss_id'].nunique()
    print(f"Number of unique classes: {num_classes}")

    # Convert integer labels to one-hot encoded vectors (required for categorical_crossentropy)
    y_train = to_categorical(y_train_ids, num_classes=num_classes)
    y_val = to_categorical(y_val_ids, num_classes=num_classes)
    y_test = to_categorical(y_test_ids, num_classes=num_classes) # Prepare test labels for evaluation too

    print(f"y_train (one-hot) shape: {y_train.shape}")
    print(f"y_val (one-hot) shape: {y_val.shape}")

    # Build the model
    model = build_lstm_model(
        input_shape=(SEQUENCE_LENGTH, EXPECTED_COORDS_PER_FRAME),
        num_classes=num_classes,
        lstm_units=LSTM_UNITS,
        dense_units=DENSE_UNITS,
        dropout_rate=DROPOUT_RATE,
        learning_rate=LEARNING_RATE
    )
    model.summary()

    # Define Callbacks for efficient training
    # 1. Early Stopping: Stop training if validation accuracy doesn't improve for 'patience' epochs
    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20, restore_best_weights=True)
    # 2. Model Checkpoint: Save the best model weights based on validation accuracy
    checkpoint_filepath = os.path.join(MODEL_SAVE_DIR, 'best_sign_classifier_model.keras') # .keras is the new recommended format
    model_checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', save_best_only=True, verbose=1)
    # 3. Reduce Learning Rate on Plateau: Reduce learning rate if validation accuracy plateaus
    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.00001, verbose=1)


    print(f"\nStarting model training with BATCH_SIZE={BATCH_SIZE} and EPOCHS={EPOCHS}...")
    history = model.fit(
        X_train, y_train,
        epochs=EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_val, y_val),
        callbacks=[early_stopping, model_checkpoint, reduce_lr],
        verbose=1
    )

    print("\n--- Training Finished ---")

    # Load the best model weights for final evaluation
    if os.path.exists(checkpoint_filepath):
        print(f"Loading best model from: {checkpoint_filepath}")
        model.load_weights(checkpoint_filepath)
    else:
        print(f"Warning: Best model checkpoint not found at {checkpoint_filepath}. Using final trained weights.")

    # Evaluate the model on the test set
    print("\nEvaluating model on the test set...")
    loss, accuracy = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)
    print(f"Test Loss: {loss:.4f}")
    print(f"Test Accuracy: {accuracy:.4f}")

    print("\nModel training and evaluation complete.")
    print(f"Best model weights saved to: {checkpoint_filepath}")